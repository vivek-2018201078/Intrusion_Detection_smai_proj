{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/kddcup.data_10_percent_corrected')\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_index = [1,2,3,6,11,20,21]\n",
    "unique_list = []\n",
    "\n",
    "unique1 = data.iloc[:,1].unique()\n",
    "unique_list.append(unique1)\n",
    "unique2 = data.iloc[:,2].unique()\n",
    "unique_list.append(unique2)\n",
    "unique3 = data.iloc[:,3].unique()\n",
    "unique_list.append(unique3)\n",
    "unique4 = data.iloc[:,6].unique()\n",
    "unique_list.append(unique4)\n",
    "unique5 = data.iloc[:,11].unique()\n",
    "unique_list.append(unique5)\n",
    "unique6 = data.iloc[:,20].unique()\n",
    "unique_list.append(unique6)\n",
    "unique7 = data.iloc[:,21].unique()\n",
    "unique_list.append(unique7)\n",
    "#print(unique_list)\n",
    "#print(len(unique_list))\n",
    "#print(unique2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0\n",
    "for i in range(len(unique1)):\n",
    "    data.iloc[:,1] = np.where(data.iloc[:,1]==unique1[i], value, data.iloc[:,1])\n",
    "    value = value + 1\n",
    "\n",
    "value = 0\n",
    "for i in range(len(unique2)):\n",
    "    data.iloc[:,2] = np.where(data.iloc[:,2]==unique2[i], value, data.iloc[:,2])\n",
    "    value = value + 1\n",
    "\n",
    "value = 0\n",
    "for i in range(len(unique3)):\n",
    "    data.iloc[:,3] = np.where(data.iloc[:,3]==unique3[i], value, data.iloc[:,3])\n",
    "    value = value + 1\n",
    "\n",
    "value = 0\n",
    "for i in range(len(unique4)):\n",
    "    data.iloc[:,6] = np.where(data.iloc[:,6]==unique4[i], value, data.iloc[:,6])\n",
    "    value = value + 1\n",
    "\n",
    "value = 0\n",
    "for i in range(len(unique5)):\n",
    "    data.iloc[:,11] = np.where(data.iloc[:,11]==unique5[i], value, data.iloc[:,11])\n",
    "    value = value + 1\n",
    "\n",
    "value = 0\n",
    "for i in range(len(unique6)):\n",
    "    data.iloc[:,20] = np.where(data.iloc[:,20]==unique6[i], value, data.iloc[:,20])\n",
    "    value = value + 1\n",
    "\n",
    "value = 0\n",
    "for i in range(len(unique7)):\n",
    "    data.iloc[:,21] = np.where(data.iloc[:,21]==unique7[i], value, data.iloc[:,21])\n",
    "    value = value + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal.' 'buffer_overflow.' 'loadmodule.' 'perl.' 'neptune.' 'smurf.'\n",
      " 'guess_passwd.' 'pod.' 'teardrop.' 'portsweep.' 'ipsweep.' 'land.'\n",
      " 'ftp_write.' 'back.' 'imap.' 'satan.' 'phf.' 'nmap.' 'multihop.'\n",
      " 'warezmaster.' 'warezclient.' 'spy.' 'rootkit.']\n",
      "494020\n",
      "['normal' 'u2r' 'DoS' 'r2l' 'probe']\n"
     ]
    }
   ],
   "source": [
    "label_values = data.iloc[:,41].unique()\n",
    "print(label_values)\n",
    "mapping = {}\n",
    "mapping['smurf.'] = 'DoS'\n",
    "mapping['neptune.'] = 'DoS'\n",
    "mapping['back.'] = 'DoS'\n",
    "mapping['satan.'] = 'probe'\n",
    "mapping['ipsweep.'] = 'probe'\n",
    "mapping['portsweep.'] = 'probe'\n",
    "mapping['warezclient.'] = 'r2l'\n",
    "mapping['teardrop.'] = 'DoS'\n",
    "mapping['pod.'] = 'DoS'\n",
    "mapping['nmap.'] = 'probe'\n",
    "mapping['guess_passwd.'] = 'r2l'\n",
    "mapping['buffer_overflow.'] = 'u2r'\n",
    "mapping['land.'] = 'DoS'\n",
    "mapping['warezmaster.'] = 'r2l'\n",
    "mapping['imap.'] = 'r2l'\n",
    "mapping['rootkit.'] = 'u2r'\n",
    "mapping['loadmodule.'] = 'u2r'\n",
    "mapping['ftp_write.'] = 'r2l'\n",
    "mapping['multihop.'] = 'r2l'\n",
    "mapping['phf.'] = 'r2l'\n",
    "mapping['perl.'] = 'u2r'\n",
    "mapping['spy.'] = 'r2l'\n",
    "mapping['normal.'] = 'normal'\n",
    "mapping['normal'] = 'normal'\n",
    "\n",
    "row_count = data.shape[0]\n",
    "print(row_count)\n",
    "label_list = []\n",
    "for i in range(row_count):\n",
    "    value = data.iloc[i][41]\n",
    "    label_list.append(mapping[value])\n",
    "\n",
    "data.drop(data.columns[41], axis=1, inplace=True)\n",
    "data.insert(41,41,value = label_list)\n",
    "\n",
    "#data.replace(to_replace = ['smurf.','neptune.','back.','teardrop.','pod.','land.'],value = 'Dos')\n",
    "#data.replace(to_replace=['satan.','ipsweep.','portsweep.','nmap.'],value = 'probe')\n",
    "#data.replace(to_replace=['spy.','phf.','multihop.','ftp_write.','imap,','warezmaster.','guess_passwd.','warezclient.'],value = 'r2l')\n",
    "#data.replace(to_replace=['loadmodule.','rootkit.','buffer_overflow.','nmap.'],value='u2r')\n",
    "#data.replace(to_replace=['normal.'],value='normal')\n",
    "\n",
    "\n",
    "#data.iloc[:,41] = data.iloc[:,41].where(data.iloc[:,41].isin(['smurf.','neptune.','back.','teardrop.','pod.','land.']),'DoS')\n",
    "#data.iloc[:,41] = data.iloc[:,41].where(data.iloc[:,41].isin(['satan.','ipsweep.','portsweep.','nmap.']),'probe')\n",
    "#data.iloc[:,41] = data.iloc[:,41].where(data.iloc[:,41].isin(['spy.','phf.','multihop.','ftp_write.','imap,','warezmaster.','guess_passwd.','warezclient.']),'r2l')\n",
    "#data.iloc[:,41] = data.iloc[:,41].where(data.iloc[:,41].isin(['loadmodule.','rootkit.','buffer_overflow.','nmap.']),'u2r')\n",
    "#data.iloc[:,41] = data.iloc[:,41].where(data.iloc[:,41].isin(['normal.']),'normal')\n",
    "\n",
    "print(data.iloc[:,41].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# data.iloc[:,41] = np.where(data.iloc[:,41] != 'normal.',1,data.iloc[:,41])\n",
    "# data.iloc[:,41] = np.where(data.iloc[:,41] == 'normal.',0,data.iloc[:,41])\n",
    "\n",
    "# data.iloc[:,41].unique()\n",
    "#print(data)\n",
    "\n",
    "data.iloc[:,41] = np.where(data.iloc[:,41] == 'normal',0,data.iloc[:,41])\n",
    "data.iloc[:,41] = np.where(data.iloc[:,41] == 'u2r',1,data.iloc[:,41])\n",
    "data.iloc[:,41] = np.where(data.iloc[:,41] == 'DoS',2,data.iloc[:,41])\n",
    "data.iloc[:,41] = np.where(data.iloc[:,41] == 'r2l',3,data.iloc[:,41])\n",
    "data.iloc[:,41] = np.where(data.iloc[:,41] == 'probe',4,data.iloc[:,41])\n",
    "\n",
    "print(data.iloc[:,41].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494020, 42)\n",
      "(494020,)\n",
      "(494020, 41)\n"
     ]
    }
   ],
   "source": [
    "# Separating Data and the Output labels into data and Y respectively\n",
    "\n",
    "print(data.shape)\n",
    "Y = data.iloc[:,41]\n",
    "Y = np.array(Y)\n",
    "#Y = Y.reshape(Y.shape[0],1)\n",
    "print(Y.shape)\n",
    "data.drop(data.columns[41], axis=1, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train :  (395216, 41)\n",
      "X_test :  (98804, 41)\n",
      "Y_train :  (395216,)\n",
      "Y_test :  (98804,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting Data into train and test\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, Y, test_size = 0.20) \n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "#Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
    "Y_test = np.array(Y_test)\n",
    "#Y_test = Y_test.reshape(Y_test.shape[0],1)\n",
    "\n",
    "print(\"X_train : \",X_train.shape)\n",
    "print(\"X_test : \",X_test.shape)\n",
    "print(\"Y_train : \",Y_train.shape)\n",
    "print(\"Y_test : \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape :  (395216, 41)\n",
      "Y_train Shape :  (395216,)\n",
      "X_train Shape :  (395216, 41)\n",
      "Y_test Shape :  (98804,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing in order to do the computation faster doing scaling can use MinMaxScaler as well...\n",
    "# Check how SVM learns in terms of distance calculation\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "X_train = X_train.astype(float)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "print(\"X_train Shape : \", X_train.shape)\n",
    "\n",
    "Y_train = np.array(Y_train)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train)\n",
    "print(\"Y_train Shape : \",Y_train.shape)\n",
    "\n",
    "X_test = X_test.astype(float)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"X_train Shape : \", X_train.shape)\n",
    "\n",
    "Y_test = encoder.transform(Y_test)\n",
    "print(\"Y_test Shape : \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Setting Up Model..\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 80.6min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 250.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Fitting..\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#flatten_Y_train = Y_train.ravel()\n",
    "#class_weights = class_weight.compute_class_weight('balanced' ,np.unique(Y_train) ,flatten_Y_train)\n",
    "grid_param = {\n",
    "    'kernel': ['rbf','linear'],\n",
    "    'C' : [1,10,100],\n",
    "    'gamma':['auto','scale']\n",
    "}\n",
    "\n",
    "svm_model = GridSearchCV(SVC(class_weight='balanced'),param_grid=grid_param,cv=5,n_jobs=-1,verbose = 1)\n",
    "print(\"Done Setting Up Model..\")\n",
    "svm_model.fit(X_train, Y_train)\n",
    "print(\"Done Fitting..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for training data: 0.9991068175377515 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: rbf \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "# View the best parameters for the model found using grid search\n",
    "print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "#print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19419     3     2    70    17]\n",
      " [   15     2     0     0     1]\n",
      " [  383     0 77886     0     0]\n",
      " [   30     0     0   182     0]\n",
      " [    6     0     0     0   788]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     19511\n",
      "           1       0.40      0.11      0.17        18\n",
      "           2       1.00      1.00      1.00     78269\n",
      "           3       0.72      0.86      0.78       212\n",
      "           4       0.98      0.99      0.98       794\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     98804\n",
      "   macro avg       0.82      0.79      0.79     98804\n",
      "weighted avg       0.99      0.99      0.99     98804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "final_model = svm_model.best_estimator_\n",
    "Y_pred = final_model.predict(X_test)\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(\"\\n\")\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# predicted = predicted.astype(int)\n",
    "# conf = confusion_matrix(Y_test,predicted)\n",
    "# print(conf)\n",
    "\n",
    "# precision,recall,f1,support = precision_recall_fscore_support(Y_test,predicted)\n",
    "# print(\"Precision : \",precision)\n",
    "# print(\"Recall : \",recall)\n",
    "# print(\"f1 : \",f1)\n",
    "# print(\"Support : \",support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 occurence in Predicted List :  19853\n",
      "1 occurence in Predicted List :  5\n",
      "2 occurence in Predicted List :  77888\n",
      "3 occurence in Predicted List :  252\n",
      "4 occurence in Predicted List :  806\n"
     ]
    }
   ],
   "source": [
    "pred = Y_pred.tolist()\n",
    "print(\"0 occurence in Predicted List : \",pred.count(0))\n",
    "print(\"1 occurence in Predicted List : \",pred.count(1))\n",
    "print(\"2 occurence in Predicted List : \",pred.count(2))\n",
    "print(\"3 occurence in Predicted List : \",pred.count(3))\n",
    "print(\"4 occurence in Predicted List : \",pred.count(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 occurence in Actual List :  19511\n",
      "1 occurence in Actual List :  18\n",
      "2 occurence in Actual List :  78269\n",
      "3 occurence in Actual List :  212\n",
      "4 occurence in Actual List :  794\n"
     ]
    }
   ],
   "source": [
    "actual = Y_test.tolist()\n",
    "print(\"0 occurence in Actual List : \",actual.count(0))\n",
    "print(\"1 occurence in Actual List : \",actual.count(1))\n",
    "print(\"2 occurence in Actual List : \",actual.count(2))\n",
    "print(\"3 occurence in Actual List : \",actual.count(3))\n",
    "print(\"4 occurence in Actual List : \",actual.count(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
