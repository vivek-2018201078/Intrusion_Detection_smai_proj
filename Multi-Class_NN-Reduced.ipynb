{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.utils import class_weight\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('../input/kddcup.data_10_percent_corrected')\n",
    "#data\n",
    "labels = pd.read_csv(\"labels.csv\",sep = \":\",header = None)\n",
    "data   = pd.read_csv(\"kddcup.data_10_percent_corrected\", names = labels.iloc[:,0].values)\n",
    "data_onehotencoded = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_index = [1,2,3,6,11,20,21]\n",
    "# unique_list = []\n",
    "\n",
    "# unique1 = data.iloc[:,1].unique()\n",
    "# unique_list.append(unique1)\n",
    "# unique2 = data.iloc[:,2].unique()\n",
    "# unique_list.append(unique2)\n",
    "# unique3 = data.iloc[:,3].unique()\n",
    "# unique_list.append(unique3)\n",
    "# unique4 = data.iloc[:,6].unique()\n",
    "# unique_list.append(unique4)\n",
    "# unique5 = data.iloc[:,11].unique()\n",
    "# unique_list.append(unique5)\n",
    "# unique6 = data.iloc[:,20].unique()\n",
    "# unique_list.append(unique6)\n",
    "# unique7 = data.iloc[:,21].unique()\n",
    "# unique_list.append(unique7)\n",
    "# #print(unique_list)\n",
    "# #print(len(unique_list))\n",
    "# #print(unique2)\n",
    "lb_make = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "cat_columns = labels.loc[labels.iloc[:,1] == \" symbolic.\",0].values\n",
    "data[cat_columns] = data[cat_columns].apply(lambda col: lb_make.fit_transform(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal' 'u2r' 'dos' 'r2l' 'probe']\n"
     ]
    }
   ],
   "source": [
    "replacements = {\n",
    "        r'(smurf.|neptune.|back.|teardrop.|pod.|land.)' : 'dos',\n",
    "        r'(normal.)' : 'normal',\n",
    "        r'(satan.|ipsweep.|portsweep.|nmap.)' : 'probe',\n",
    "        r'(warezclient.|guess_passwd.|warezmaster.|imap.|ftp_write.|multihop.|phf.|spy.)' : 'r2l',\n",
    "        r'(buffer_overflow.|rootkit.|loadmodule.|perl.)' : 'u2r'\n",
    "    }\n",
    "data.replace(replacements, regex=True, inplace=True)\n",
    "print(data.iloc[:,41].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffr(df, t):\n",
    "    classes = ['normal', 'u2r', 'dos', 'r2l', 'probe']\n",
    "    df_new = df.iloc[:,:-1]\n",
    "    var_d = []\n",
    "    for clas in classes:\n",
    "        temp = df.loc[df['output'] == clas]\n",
    "        #print(\"is unique \", temp.output.unique())\n",
    "        temp = temp.iloc[:,:-1].values\n",
    "        #print(\"temp = \", temp)\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        temp_scaled = min_max_scaler.fit_transform(temp)\n",
    "        #print(\"temp_scaled = \", temp_scaled)\n",
    "        mean_ind = temp_scaled.mean(axis = 0)\n",
    "        #print(\"mean_ind = \", mean_ind)\n",
    "        var_d_f = np.square(mean_ind - temp).mean(axis = 0)\n",
    "        #print(\"var_d_f_f = \", var_d_f)\n",
    "        var_d.append(var_d_f)\n",
    "    var_d = np.array(var_d)\n",
    "    #print(var_d)\n",
    "    var_d_mean = var_d.mean(axis = 0)\n",
    "    \n",
    "    \n",
    "#     var = np.zeros(len(mean_means))\n",
    "#     for i in means:\n",
    "#         var += np.square(i - mean_means)\n",
    "#     var /= len(mean_means)\n",
    "#     #print(var)\n",
    "    indexes = list(np.argsort(var_d_mean))\n",
    "    filtered_indexes = indexes[:t]\n",
    "    filtered_indexes.append(41)\n",
    "    filtered_data = df.iloc[:, filtered_indexes]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "def NN(data,feature):\n",
    "    epoch = 100\n",
    "    batchsize = 32\n",
    "\n",
    "    data = data.sample(frac=1,random_state=200)\n",
    "    Input1 = data.iloc[:,:-1]\n",
    "    Output1 = data.iloc[:,-1]\n",
    "    Y = np.array(Output1)\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "    print(str(encoder.classes_))\n",
    "    Input1 = np.array(Input1)\n",
    "    Y = np_utils.to_categorical(encoded_Y)\n",
    "    print(Input1.shape)\n",
    "    print(Y.shape)\n",
    "    #print(Output1.shape)\n",
    "    print(type(Input1))\n",
    "    print(type(Output1))\n",
    "    Input1 = Input1.astype(float)\n",
    "    split = int(math.ceil(0.8 * len(Input1)))\n",
    "    X_train = Input1[0:split]\n",
    "    Y_train = Y[0:split]\n",
    "\n",
    "    X_test = Input1[split:]\n",
    "    Y_test = Y[split:]\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(Y_train.shape)\n",
    "    print(Y_test.shape)\n",
    "    flatten_Y_train = Y_train.ravel()\n",
    "    class_weights = class_weight.compute_class_weight('balanced' ,np.unique(Y_train) ,flatten_Y_train)\n",
    "    \n",
    "    \n",
    "    inp = Input(shape=(feature,))\n",
    "    hidden1 = Dense(40,activation='sigmoid')(inp)\n",
    "    hidden2 = Dense(40,activation='sigmoid')(hidden1)\n",
    "    out = Dense(5,activation='softmax')(hidden2)\n",
    "\n",
    "    model = Model(inp,out)\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "    model.summary()\n",
    "\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(X_train,Y_train,epochs = epoch,batch_size=batchsize,class_weight=class_weights)\n",
    "    end = time.time()\n",
    "    print(\"Training Time for Multi-Class NN : \",(end - start))\n",
    "    \n",
    "    predicted = model.predict(X_test)\n",
    "    #predicted = predicted.ravel()\n",
    "    print(predicted.shape)\n",
    "    print(predicted)\n",
    "    #print(len(predicted))\n",
    "    Y_predicted = np.empty((len(predicted),))\n",
    "    for i in range(len(predicted)):\n",
    "        Y_predicted[i] = np.argmax(predicted[i])\n",
    "\n",
    "    print(Y_predicted)\n",
    "    #y_pre,y_pred_counts = np.unique(Y_predicted,return_counts=True)\n",
    "    #print(y_pre,y_pred_counts)\n",
    "\n",
    "    # Y_predicted = Y_predicted.astype(int)\n",
    "\n",
    "\n",
    "    Y_test_final = np.empty((len(Y_test),))\n",
    "    for i in range(len(Y_test)):\n",
    "        Y_test_final[i] = np.argmax(Y_test[i])\n",
    "\n",
    "\n",
    "    print(Y_predicted.shape)\n",
    "    print(Y_test_final.shape)\n",
    "    print(type(Y_predicted))\n",
    "    print(type(Y_test_final))\n",
    "\n",
    "    print(\"Y_predicted Unique and its Frequency :\")\n",
    "    y_pre,y_pred_counts = np.unique(Y_predicted,return_counts=True)\n",
    "    print(y_pre,y_pred_counts)\n",
    "\n",
    "    print(\"Y_test Final unique and its Frequency : \")\n",
    "    y_test_final,y_test_counts = np.unique(Y_test_final,return_counts=True)\n",
    "    print(y_test_final,y_test_counts)\n",
    "\n",
    "    conf = confusion_matrix(Y_test_final,Y_predicted)\n",
    "    print(conf)\n",
    "\n",
    "    precision,recall,f1,support = precision_recall_fscore_support(Y_test_final,Y_predicted)\n",
    "    print(\"Precision : \",precision)\n",
    "    print(\"Recall : \",recall)\n",
    "    print(\"f1 : \",f1)\n",
    "    print(\"Support : \",support)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "for i in range(10,31,10):\n",
    "    new_dat = ffr(data, i)\n",
    "    print(\"features = \", i)\n",
    "    print(new_dat.shape)\n",
    "    print(type(new_dat))\n",
    "    print(\"With Feature size : \", i)\n",
    "    NN(new_dat,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
