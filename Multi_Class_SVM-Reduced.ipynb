{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('../input/kddcup.data_10_percent_corrected')\n",
    "#data\n",
    "labels = pd.read_csv(\"labels.csv\",sep = \":\",header = None)\n",
    "data   = pd.read_csv(\"kddcup.data_10_percent_corrected\", names = labels.iloc[:,0].values)\n",
    "data_onehotencoded = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "cat_columns = labels.loc[labels.iloc[:,1] == \" symbolic.\",0].values\n",
    "data[cat_columns] = data[cat_columns].apply(lambda col: lb_make.fit_transform(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "        r'(smurf.|neptune.|back.|teardrop.|pod.|land.)' : 'dos',\n",
    "        r'(normal.)' : 'normal',\n",
    "        r'(satan.|ipsweep.|portsweep.|nmap.)' : 'probe',\n",
    "        r'(warezclient.|guess_passwd.|warezmaster.|imap.|ftp_write.|multihop.|phf.|spy.)' : 'r2l',\n",
    "        r'(buffer_overflow.|rootkit.|loadmodule.|perl.)' : 'u2r'\n",
    "    }\n",
    "data.replace(replacements, regex=True, inplace=True)\n",
    "print(data.iloc[:,41].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffr(df, t):\n",
    "    classes = ['normal', 'u2r', 'dos', 'r2l', 'probe']\n",
    "    df_new = df.iloc[:,:-1]\n",
    "    var_d = []\n",
    "    for clas in classes:\n",
    "        temp = df.loc[df['output'] == clas]\n",
    "        #print(\"is unique \", temp.output.unique())\n",
    "        temp = temp.iloc[:,:-1].values\n",
    "        #print(\"temp = \", temp)\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        temp_scaled = min_max_scaler.fit_transform(temp)\n",
    "        #print(\"temp_scaled = \", temp_scaled)\n",
    "        mean_ind = temp_scaled.mean(axis = 0)\n",
    "        #print(\"mean_ind = \", mean_ind)\n",
    "        var_d_f = np.square(mean_ind - temp).mean(axis = 0)\n",
    "        #print(\"var_d_f_f = \", var_d_f)\n",
    "        var_d.append(var_d_f)\n",
    "    var_d = np.array(var_d)\n",
    "    #print(var_d)\n",
    "    var_d_mean = var_d.mean(axis = 0)\n",
    "    \n",
    "    \n",
    "#     var = np.zeros(len(mean_means))\n",
    "#     for i in means:\n",
    "#         var += np.square(i - mean_means)\n",
    "#     var /= len(mean_means)\n",
    "#     #print(var)\n",
    "    indexes = list(np.argsort(var_d_mean))\n",
    "    filtered_indexes = indexes[:t]\n",
    "    filtered_indexes.append(41)\n",
    "    filtered_data = df.iloc[:, filtered_indexes]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ffr(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating Data and the Output labels into data and Y respectively\n",
    "\n",
    "print(data.shape)\n",
    "Y = data.iloc[:,-1]\n",
    "Y = np.array(Y)\n",
    "#Y = Y.reshape(Y.shape[0],1)\n",
    "print(Y.shape)\n",
    "data.drop(data.columns[-1], axis=1, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data into train and test\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, Y, test_size = 0.20) \n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "#Y_train = Y_train.reshape(Y_train.shape[0],1)\n",
    "Y_test = np.array(Y_test)\n",
    "#Y_test = Y_test.reshape(Y_test.shape[0],1)\n",
    "\n",
    "print(\"X_train : \",X_train.shape)\n",
    "print(\"X_test : \",X_test.shape)\n",
    "print(\"Y_train : \",Y_train.shape)\n",
    "print(\"Y_test : \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing in order to do the computation faster doing scaling can use MinMaxScaler as well...\n",
    "# Check how SVM learns in terms of distance calculation\n",
    "\n",
    "X_train = X_train.astype(float)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "print(\"X_train Shape : \", X_train.shape)\n",
    "\n",
    "Y_train = np.array(Y_train)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "Y_train = encoder.transform(Y_train)\n",
    "print(\"Y_train Shape : \",Y_train.shape)\n",
    "\n",
    "X_test = X_test.astype(float)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"X_train Shape : \", X_train.shape)\n",
    "\n",
    "Y_test = encoder.transform(Y_test)\n",
    "print(\"Y_test Shape : \",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#flatten_Y_train = Y_train.ravel()\n",
    "#class_weights = class_weight.compute_class_weight('balanced' ,np.unique(Y_train) ,flatten_Y_train)\n",
    "# grid_param = {\n",
    "#     'kernel': ['rbf','linear'],\n",
    "#     'C' : [1,10,100],\n",
    "#     'gamma':['auto','scale']\n",
    "# }\n",
    "\n",
    "# svm_model = GridSearchCV(SVC(class_weight='balanced'),param_grid=grid_param,cv=5,n_jobs=-1,verbose = 1)\n",
    "# print(\"Done Setting Up Model..\")\n",
    "# svm_model.fit(X_train, Y_train)\n",
    "# print(\"Done Fitting..\")\n",
    "\n",
    "\n",
    "svm_model = svm.SVC(kernel='rbf',C=100)\n",
    "svm_model.fit(X_train,Y_train)\n",
    "print(\"Done Fitting..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "# # View the best parameters for the model found using grid search\n",
    "# print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "# print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "# #print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_model = svm_model.best_estimator_\n",
    "Y_pred = svm_model.predict(X_test)\n",
    "print(confusion_matrix(Y_test,Y_pred))\n",
    "print(\"\\n\")\n",
    "print(classification_report(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# predicted = predicted.astype(int)\n",
    "# conf = confusion_matrix(Y_test,predicted)\n",
    "# print(conf)\n",
    "\n",
    "# precision,recall,f1,support = precision_recall_fscore_support(Y_test,predicted)\n",
    "# print(\"Precision : \",precision)\n",
    "# print(\"Recall : \",recall)\n",
    "# print(\"f1 : \",f1)\n",
    "# print(\"Support : \",support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = Y_pred.tolist()\n",
    "print(\"0 occurence in Predicted List : \",pred.count(0))\n",
    "print(\"1 occurence in Predicted List : \",pred.count(1))\n",
    "print(\"2 occurence in Predicted List : \",pred.count(2))\n",
    "print(\"3 occurence in Predicted List : \",pred.count(3))\n",
    "print(\"4 occurence in Predicted List : \",pred.count(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = Y_test.tolist()\n",
    "print(\"0 occurence in Actual List : \",actual.count(0))\n",
    "print(\"1 occurence in Actual List : \",actual.count(1))\n",
    "print(\"2 occurence in Actual List : \",actual.count(2))\n",
    "print(\"3 occurence in Actual List : \",actual.count(3))\n",
    "print(\"4 occurence in Actual List : \",actual.count(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
